#prometheus_treecache_zookeeper_failures_total - ZooKeeper failures
#prometheus_target_scrapes_sample_out_of_order_total - rejected OutOfOrder samples
#prometheus_target_scrapes_sample_out_of_bounds_total - rejescted samples OutOfTimebounds
#prometheus_target_scrapes_sample_duplicate_timestamp_total - rejected duplicated samples with different values
#prometheus_target_scrapes_exceeded_sample_limit_total - rejected scrapes due to sample limit

#prometheus_sd_triton_refresh_failures_total - triton scrape failures
#prometheus_sd_openstack_refresh_failures_total - Openstack scrape failures
#prometheus_sd_marathon_refresh_failures_total - Marathon refresh failures
#prometheus_sd_gce_refresh_failures_total - GCE refresh failures
#prometheus_sd_file_read_errors_total - File discovery read error
#prometheus_sd_ec2_refresh_failures_total - EC2 discovery failures
#prometheus_sd_dns_lookup_failures_total - DNS discovery failures
#prometheus_sd_consul_rpc_failures_total - Consul discovery failures
#prometheus_sd_azure_refresh_failures_total - Azure discovery failures

#prometheus_rule_group_iterations_missed_total - Group iteration missed
#prometheus_rule_evaluation_failures_total - Rule evaluation failure

groups:
- name: Prometheus rules
  rules:
  - alert: PrometheusConfigReloadFailed
    expr: prometheus_config_last_reload_successful == 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Reloading Prometheus' configuration has failed for {{$labels.instance}}
      summary: Prometheus reload failed {{$labels.instance}}
  - alert: PrometheusNotificationQueueRunningFull
    expr: predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > prometheus_notifications_queue_capacity
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Prometheus' alert notification queue is running full for {{$labels.instance}}
      summary: Prometheus alert queue is running full {{$labels.instance}}
  - alert: PrometheusErrorSendingAlerts
    expr: delta(prometheus_notifications_errors_total[5m]) > 0
    labels:
      severity: critical
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.instance}} to Alertmanager {{$labels.Alertmanager}}
      summary: Errors while sending allerts {{$labels.instance}} {{$labels.Alertmanager}}
  - alert: PrometheusErrorSendingAlerts
    expr: delta(prometheus_notifications_errors_total[5m]) > 0
    labels:
      severity: critical
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.instance}} to Alertmanager {{$labels.Alertmanager}}
      summary: Errors while sending allerts {{$labels.instance}} {{$labels.Alertmanager}}
  - alert: PrometheusNotConnectedToAlertmanagers
    expr: prometheus_notifications_dropped_total > 0
    labels:
      severity: warning
    annotations:
      description: "Prometheus {{ $labels.instance }} dropped {{ $value }} alerts. They will never come back"
      summary: "{{ $labels.instance }} dropped {{ $value }} alerts"
  - alert: PrometheusTSDBReloadsFailing
    expr: increase(prometheus_tsdb_reloads_failures_total[5m]) > 0
    for: 30m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} reload failures over the last four hours.'
      summary: Prometheus {{$labels.instance}} has issues reloading data blocks from disk
  - alert: PrometheusTSDBCompactionsFailing
    expr: increase(prometheus_tsdb_compactions_failed_total[5m]) > 0
    for: 30m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} compaction failures over the last four hours.'
      summary: Prometheus {{$labels.instance}} has issues compacting sample blocks
  - alert: PrometheusTSDBWALCorruptions
    expr: tsdb_wal_corruptions_total > 0
    for: 30m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead log (WAL).'
      summary: Prometheus write-ahead log is corrupted o {{ $labels.instance }}
