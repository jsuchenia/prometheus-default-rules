#prometheus_target_scrapes_sample_out_of_order_total - rejected OutOfOrder samples
#prometheus_target_scrapes_sample_out_of_bounds_total - rejescted samples OutOfTimebounds
#prometheus_target_scrapes_sample_duplicate_timestamp_total - rejected duplicated samples with different values
#prometheus_target_scrapes_exceeded_sample_limit_total - rejected scrapes due to sample limit

groups:
- name: Prometheus rules
  rules:
  - alert: PrometheusConfigReloadFailed
    expr: prometheus_config_last_reload_successful == 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Reloading Prometheus' configuration has failed for {{$labels.instance}}
      summary: Prometheus reload failed {{$labels.instance}}
  - alert: PrometheusNotificationQueueRunningFull
    expr: predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > prometheus_notifications_queue_capacity
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Prometheus' alert notification queue is running full for {{$labels.instance}}
      summary: Prometheus alert queue is running full {{$labels.instance}}
  - alert: PrometheusErrorSendingAlerts
    expr: delta(prometheus_notifications_errors_total[5m]) > 0
    labels:
      severity: critical
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.instance}} to Alertmanager {{$labels.Alertmanager}}
      summary: Errors while sending allerts {{$labels.instance}} {{$labels.Alertmanager}}
  - alert: PrometheusErrorSendingAlerts
    expr: delta(prometheus_notifications_errors_total[5m]) > 0
    labels:
      severity: critical
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.instance}} to Alertmanager {{$labels.Alertmanager}}
      summary: Errors while sending allerts {{$labels.instance}} {{$labels.Alertmanager}}
  - alert: PrometheusNotConnectedToAlertmanagers
    expr: prometheus_notifications_dropped_total > 0
    labels:
      severity: warning
    annotations:
      description: "Prometheus {{ $labels.instance }} dropped {{ $value }} alerts. They will never come back"
      summary: "{{ $labels.instance }} dropped {{ $value }} alerts"
  - alert: PrometheusTSDBReloadsFailing
    expr: increase(prometheus_tsdb_reloads_failures_total[5m]) > 0
    for: 30m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} reload failures over the last four hours.'
      summary: Prometheus {{$labels.instance}} has issues reloading data blocks from disk
  - alert: PrometheusTSDBCompactionsFailing
    expr: increase(prometheus_tsdb_compactions_failed_total[5m]) > 0
    for: 30m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} compaction failures over the last four hours.'
      summary: Prometheus {{$labels.instance}} has issues compacting sample blocks
  - alert: PrometheusTSDBWALCorruptions
    expr: tsdb_wal_corruptions_total > 0
    for: 30m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead log (WAL).'
      summary: Prometheus write-ahead log is corrupted o {{ $labels.instance }}
  - alert: PrometheusRuleEvaluationFailures
    expr: delta(prometheus_rule_evaluation_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports rule evaulation error'
      summary: There were some errors during evaluation of rules by {{$labels.job }} at {{ $labels.instance }}. Please validate system cpu, storage performance of this instance
  - alert: PrometheusRuleGroupMissed
    expr: delta(prometheus_rule_group_iterations_missed_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} skipped some groups'
      summary: There were some errors during evaluation of rules by {{$labels.job }} at {{ $labels.instance }} - some rules groups were skipped. Please validate system cpu, storage performance of this instance
  - alert: PrometheusZooKeeperFailures
    expr: delta(prometheus_treecache_zookeeper_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with zookeeper'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with zookeeper instance. Please check zookeeper health and networking connectivity
  - alert: PrometheusTritonFailures
    expr: delta(prometheus_sd_triton_refresh_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with triton service'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with triton instance. Please check triton service health and networking connectivity
  - alert: PrometheusOpenstackFailures
    expr: delta(prometheus_sd_openstack_refresh_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with openstack metadata'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with openstack discovery service. Please check openstack master servers and networking connectivity
  - alert: PrometheusMarathonFailures
    expr: delta(prometheus_sd_marathon_refresh_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with marathon service'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with marathon discovery service. Please check marathon servers and networking connectivity
  - alert: PrometheusGCEFailures
    expr: delta(prometheus_sd_gce_refresh_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with GCE service'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with Google Compute Engine discovery. Please check GCE systems and networking connectivity
  - alert: PrometheusFileReadFailures
    expr: delta(prometheus_sd_file_read_errors_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with file read discovery'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with file read discovery. Please check storage performance, CPU and networking connectivity
  - alert: PrometheusEc2Failures
    expr: delta(prometheus_sd_ec2_refresh_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with AWS EC2 discovery'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with AWS EC2 discovery service. Please check API availability, CPU and networking connectivity
  - alert: PrometheusDNSFailures
    expr: delta(prometheus_sd_dns_lookup_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with DNS discovery'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with DNS discovery service. Please check networking connectivity and DNS status
  - alert: PrometheusConsulFailures
    expr: delta(prometheus_sd_consul_rpc_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with Consul discovery'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with Consul discovery service. Please check networking connectivity and Consul service status
  - alert: PrometheusAzureFailures
    expr: delta(prometheus_sd_azure_refresh_failures_total[1m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} reports problem with Azure discovery'
      summary: {{$labels.job }} at {{ $labels.instance }} - reports failures with Azure discovery service. Please check networking connectivity and Azure service status
